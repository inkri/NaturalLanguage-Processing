{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "570ba77b",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c405f2a",
   "metadata": {},
   "source": [
    "## How to Clean Text Manually and with NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe6458b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text\n",
    "filename = 'pg5200.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90e1c845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï»¿The', 'Project', 'Gutenberg', 'EBook', 'of', 'Metamorphosis,', 'by', 'Franz', 'Kafka', 'Translated', 'by', 'David', 'Wyllie.', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever.', 'You', 'may', 'copy', 'it,', 'give', 'it', 'away', 'or', 're-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this', 'eBook', 'or', 'online', 'at', 'www.gutenberg.org', '**', 'This', 'is', 'a', 'COPYRIGHTED', 'Project', 'Gutenberg', 'eBook,', 'Details', 'Below', '**', '**', 'Please', 'follow', 'the', 'copyright', 'guidelines', 'in', 'this', 'file.', '**', 'Title:', 'Metamorphosis', 'Author:', 'Franz', 'Kafka', 'Translator:', 'David', 'Wyllie', 'Release', 'Date:', 'August', '16,', '2005', '[EBook', '#5200]', 'First', 'posted:', 'May', '13,', '2002', 'Last', 'updated:']\n"
     ]
    }
   ],
   "source": [
    "#Split by Whitespace\n",
    "# load text\n",
    "filename = 'pg5200.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words by white space\n",
    "words = text.split()\n",
    "print(words[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "998502b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï', 'The', 'Project', 'Gutenberg', 'EBook', 'of', 'Metamorphosis', 'by', 'Franz', 'Kafka', 'Translated', 'by', 'David', 'Wyllie', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'You', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 're', 'use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this', 'eBook', 'or', 'online', 'at', 'www', 'gutenberg', 'org', 'This', 'is', 'a', 'COPYRIGHTED', 'Project', 'Gutenberg', 'eBook', 'Details', 'Below', 'Please', 'follow', 'the', 'copyright', 'guidelines', 'in', 'this', 'file', 'Title', 'Metamorphosis', 'Author', 'Franz', 'Kafka', 'Translator', 'David', 'Wyllie', 'Release', 'Date', 'August', '16', '2005', 'EBook', '5200', 'First', 'posted', 'May', '13', '2002', 'Last', 'updated']\n"
     ]
    }
   ],
   "source": [
    "#Select Words\n",
    "import re\n",
    "# load text\n",
    "filename = 'pg5200.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split based on words only- s (a-z, A-Z, 0-9 and ‘ ’)\n",
    "words = re.split(r'\\W+', text)\n",
    "print(words[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "186fcf5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "#Split by Whitespace and Remove Punctuation\n",
    "import string\n",
    "print(string.punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5faa8585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï»¿The', 'Project', 'Gutenberg', 'EBook', 'of', 'Metamorphosis', 'by', 'Franz', 'Kafka', 'Translated', 'by', 'David', 'Wyllie', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'You', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 'reuse', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this', 'eBook', 'or', 'online', 'at', 'wwwgutenbergorg', '', 'This', 'is', 'a', 'COPYRIGHTED', 'Project', 'Gutenberg', 'eBook', 'Details', 'Below', '', '', 'Please', 'follow', 'the', 'copyright', 'guidelines', 'in', 'this', 'file', '', 'Title', 'Metamorphosis', 'Author', 'Franz', 'Kafka', 'Translator', 'David', 'Wyllie', 'Release', 'Date', 'August', '16', '2005', 'EBook', '5200', 'First', 'posted', 'May', '13', '2002', 'Last', 'updated']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "# load text\n",
    "filename = 'pg5200.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words by white space\n",
    "words = text.split()\n",
    "# prepare regex for char filtering\n",
    "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "# remove punctuation from each word\n",
    "stripped = [re_punc.sub('', w) for w in words]\n",
    "print(stripped[:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9fd4c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'EBook',\n",
       " 'of',\n",
       " 'Metamorphosis,',\n",
       " 'by',\n",
       " 'Franz',\n",
       " 'Kafka',\n",
       " 'Translated',\n",
       " 'by',\n",
       " 'David',\n",
       " 'Wyllie.',\n",
       " 'This',\n",
       " 'eBook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'no',\n",
       " 'cost',\n",
       " 'and',\n",
       " 'with',\n",
       " 'almost',\n",
       " 'no',\n",
       " 'restrictions',\n",
       " 'whatsoever.',\n",
       " 'You',\n",
       " 'may',\n",
       " 'copy',\n",
       " 'it,',\n",
       " 'give',\n",
       " 'it',\n",
       " 'away',\n",
       " 'or',\n",
       " 're-use',\n",
       " 'it',\n",
       " 'under',\n",
       " 'the',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'License',\n",
       " 'included',\n",
       " 'with',\n",
       " 'this',\n",
       " 'eBook',\n",
       " 'or',\n",
       " 'online',\n",
       " 'at',\n",
       " 'www.gutenberg.org',\n",
       " '**',\n",
       " 'This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'COPYRIGHTED',\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'eBook,',\n",
       " 'Details',\n",
       " 'Below',\n",
       " '**',\n",
       " '**',\n",
       " 'Please',\n",
       " 'follow',\n",
       " 'the',\n",
       " 'copyright',\n",
       " 'guidelines',\n",
       " 'in',\n",
       " 'this',\n",
       " 'file.',\n",
       " '**',\n",
       " 'Title:',\n",
       " 'Metamorphosis',\n",
       " 'Author:',\n",
       " 'Franz',\n",
       " 'Kafka',\n",
       " 'Translator:',\n",
       " 'David',\n",
       " 'Wyllie',\n",
       " 'Release',\n",
       " 'Date:',\n",
       " 'August',\n",
       " '16,',\n",
       " '2005',\n",
       " '[EBook',\n",
       " '#5200]',\n",
       " 'First',\n",
       " 'posted:',\n",
       " 'May',\n",
       " '13,',\n",
       " '2002',\n",
       " 'Last',\n",
       " 'updated:',\n",
       " 'May',\n",
       " '20,',\n",
       " '2012',\n",
       " 'Language:',\n",
       " 'English',\n",
       " '***',\n",
       " 'START',\n",
       " 'OF',\n",
       " 'THIS',\n",
       " 'PROJECT',\n",
       " 'GUTENBERG',\n",
       " 'EBOOK',\n",
       " 'METAMORPHOSIS',\n",
       " '***',\n",
       " 'Copyright',\n",
       " '(C)',\n",
       " '2002',\n",
       " 'David',\n",
       " 'Wyllie.',\n",
       " 'Metamorphosis',\n",
       " 'Franz',\n",
       " 'Kafka',\n",
       " 'Translated',\n",
       " 'by',\n",
       " 'David',\n",
       " 'Wyllie',\n",
       " 'I',\n",
       " 'One',\n",
       " 'morning,',\n",
       " 'when',\n",
       " 'Gregor',\n",
       " 'Samsa',\n",
       " 'woke',\n",
       " 'from',\n",
       " 'troubled',\n",
       " 'dreams,',\n",
       " 'he',\n",
       " 'found',\n",
       " 'himself',\n",
       " 'transformed',\n",
       " 'in',\n",
       " 'his',\n",
       " 'bed',\n",
       " 'into',\n",
       " 'a',\n",
       " 'horrible',\n",
       " 'vermin.',\n",
       " 'He',\n",
       " 'lay',\n",
       " 'on',\n",
       " 'his',\n",
       " 'armour-like',\n",
       " 'back,',\n",
       " 'and',\n",
       " 'if',\n",
       " 'he',\n",
       " 'lifted',\n",
       " 'his',\n",
       " 'head',\n",
       " 'a',\n",
       " 'little',\n",
       " 'he',\n",
       " 'could',\n",
       " 'see',\n",
       " 'his',\n",
       " 'brown',\n",
       " 'belly,',\n",
       " 'slightly',\n",
       " 'domed',\n",
       " 'and',\n",
       " 'divided',\n",
       " 'by',\n",
       " 'arches',\n",
       " 'into',\n",
       " 'stiff',\n",
       " 'sections.',\n",
       " 'The',\n",
       " 'bedding',\n",
       " 'was',\n",
       " 'hardly',\n",
       " 'able',\n",
       " 'to',\n",
       " 'cover',\n",
       " 'it',\n",
       " 'and',\n",
       " 'seemed',\n",
       " 'ready',\n",
       " 'to',\n",
       " 'slide',\n",
       " 'off',\n",
       " 'any',\n",
       " 'moment.',\n",
       " 'His',\n",
       " 'many',\n",
       " 'legs,',\n",
       " 'pitifully',\n",
       " 'thin',\n",
       " 'compared',\n",
       " 'with',\n",
       " 'the',\n",
       " 'size',\n",
       " 'of',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'him,',\n",
       " 'waved',\n",
       " 'about',\n",
       " 'helplessly',\n",
       " 'as',\n",
       " 'he',\n",
       " 'looked.',\n",
       " '\"What\\'s',\n",
       " 'happened',\n",
       " 'to',\n",
       " 'me?\"',\n",
       " 'he',\n",
       " 'thought.',\n",
       " 'It',\n",
       " \"wasn't\",\n",
       " 'a',\n",
       " 'dream.',\n",
       " 'His',\n",
       " 'room,',\n",
       " 'a',\n",
       " 'proper',\n",
       " 'human',\n",
       " 'room',\n",
       " 'although',\n",
       " 'a',\n",
       " 'little',\n",
       " 'too',\n",
       " 'small,',\n",
       " 'lay',\n",
       " 'peacefully',\n",
       " 'between',\n",
       " 'its',\n",
       " 'four',\n",
       " 'familiar',\n",
       " 'walls.',\n",
       " 'A',\n",
       " 'collection',\n",
       " 'of',\n",
       " 'textile',\n",
       " 'samples',\n",
       " 'lay',\n",
       " 'spread',\n",
       " 'out',\n",
       " 'on',\n",
       " 'the',\n",
       " 'table',\n",
       " '-',\n",
       " 'Samsa',\n",
       " 'was',\n",
       " 'a',\n",
       " 'travelling',\n",
       " 'salesman',\n",
       " '-',\n",
       " 'and',\n",
       " 'above',\n",
       " 'it',\n",
       " 'there',\n",
       " 'hung',\n",
       " 'a',\n",
       " 'picture',\n",
       " 'that',\n",
       " 'he',\n",
       " 'had',\n",
       " 'recently',\n",
       " 'cut',\n",
       " 'out',\n",
       " 'of',\n",
       " 'an',\n",
       " 'illustrated',\n",
       " 'magazine',\n",
       " 'and',\n",
       " 'housed',\n",
       " 'in',\n",
       " 'a',\n",
       " 'nice,',\n",
       " 'gilded',\n",
       " 'frame.',\n",
       " 'It',\n",
       " 'showed',\n",
       " 'a',\n",
       " 'lady',\n",
       " 'fitted',\n",
       " 'out',\n",
       " 'with',\n",
       " 'a',\n",
       " 'fur',\n",
       " 'hat',\n",
       " 'and',\n",
       " 'fur',\n",
       " 'boa',\n",
       " 'who',\n",
       " 'sat',\n",
       " 'upright,',\n",
       " 'raising',\n",
       " 'a',\n",
       " 'heavy',\n",
       " 'fur',\n",
       " 'muff',\n",
       " 'that',\n",
       " 'covered',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'of',\n",
       " 'her',\n",
       " 'lower',\n",
       " 'arm',\n",
       " 'towards',\n",
       " 'the',\n",
       " 'viewer.',\n",
       " 'Gregor',\n",
       " 'then',\n",
       " 'turned',\n",
       " 'to',\n",
       " 'look',\n",
       " 'out',\n",
       " 'the',\n",
       " 'window',\n",
       " 'at',\n",
       " 'the',\n",
       " 'dull',\n",
       " 'weather.',\n",
       " 'Drops',\n",
       " 'of',\n",
       " 'rain',\n",
       " 'could',\n",
       " 'be',\n",
       " 'heard',\n",
       " 'hitting',\n",
       " 'the',\n",
       " 'pane,',\n",
       " 'which',\n",
       " 'made',\n",
       " 'him',\n",
       " 'feel',\n",
       " 'quite',\n",
       " 'sad.',\n",
       " '\"How',\n",
       " 'about',\n",
       " 'if',\n",
       " 'I',\n",
       " 'sleep',\n",
       " 'a',\n",
       " 'little',\n",
       " 'bit',\n",
       " 'longer',\n",
       " 'and',\n",
       " 'forget',\n",
       " 'all',\n",
       " 'this',\n",
       " 'nonsense\",',\n",
       " 'he',\n",
       " 'thought,',\n",
       " 'but',\n",
       " 'that',\n",
       " 'was',\n",
       " 'something',\n",
       " 'he',\n",
       " 'was',\n",
       " 'unable',\n",
       " 'to',\n",
       " 'do',\n",
       " 'because',\n",
       " 'he',\n",
       " 'was',\n",
       " 'used',\n",
       " 'to',\n",
       " 'sleeping',\n",
       " 'on',\n",
       " 'his',\n",
       " 'right,',\n",
       " 'and',\n",
       " 'in',\n",
       " 'his',\n",
       " 'present',\n",
       " 'state',\n",
       " \"couldn't\",\n",
       " 'get',\n",
       " 'into',\n",
       " 'that',\n",
       " 'position.',\n",
       " 'However',\n",
       " 'hard',\n",
       " 'he',\n",
       " 'threw',\n",
       " 'himself',\n",
       " 'onto',\n",
       " 'his',\n",
       " 'right,',\n",
       " 'he',\n",
       " 'always',\n",
       " 'rolled',\n",
       " 'back',\n",
       " 'to',\n",
       " 'where',\n",
       " 'he',\n",
       " 'was.',\n",
       " 'He',\n",
       " 'must',\n",
       " 'have',\n",
       " 'tried',\n",
       " 'it',\n",
       " 'a',\n",
       " 'hundred',\n",
       " 'times,',\n",
       " 'shut',\n",
       " 'his',\n",
       " 'eyes',\n",
       " 'so',\n",
       " 'that',\n",
       " 'he',\n",
       " \"wouldn't\",\n",
       " 'have',\n",
       " 'to',\n",
       " 'look',\n",
       " 'at',\n",
       " 'the',\n",
       " 'floundering',\n",
       " 'legs,',\n",
       " 'and',\n",
       " 'only',\n",
       " 'stopped',\n",
       " 'when',\n",
       " 'he',\n",
       " 'began',\n",
       " 'to',\n",
       " 'feel',\n",
       " 'a',\n",
       " 'mild,',\n",
       " 'dull',\n",
       " 'pain',\n",
       " 'there',\n",
       " 'that',\n",
       " 'he',\n",
       " 'had',\n",
       " 'never',\n",
       " 'felt',\n",
       " 'before.',\n",
       " '\"Oh,',\n",
       " 'God\",',\n",
       " 'he',\n",
       " 'thought,',\n",
       " '\"what',\n",
       " 'a',\n",
       " 'strenuous',\n",
       " 'career',\n",
       " 'it',\n",
       " 'is',\n",
       " 'that',\n",
       " \"I've\",\n",
       " 'chosen!',\n",
       " 'Travelling',\n",
       " 'day',\n",
       " 'in',\n",
       " 'and',\n",
       " 'day',\n",
       " 'out.',\n",
       " 'Doing',\n",
       " 'business',\n",
       " 'like',\n",
       " 'this',\n",
       " 'takes',\n",
       " 'much',\n",
       " 'more',\n",
       " 'effort',\n",
       " 'than',\n",
       " 'doing',\n",
       " 'your',\n",
       " 'own',\n",
       " 'business',\n",
       " 'at',\n",
       " 'home,',\n",
       " 'and',\n",
       " 'on',\n",
       " 'top',\n",
       " 'of',\n",
       " 'that',\n",
       " \"there's\",\n",
       " 'the',\n",
       " 'curse',\n",
       " 'of',\n",
       " 'travelling,',\n",
       " 'worries',\n",
       " 'about',\n",
       " 'making',\n",
       " 'train',\n",
       " 'connections,',\n",
       " 'bad',\n",
       " 'and',\n",
       " 'irregular',\n",
       " 'food,',\n",
       " 'contact',\n",
       " 'with',\n",
       " 'different',\n",
       " 'people',\n",
       " 'all',\n",
       " 'the',\n",
       " 'time',\n",
       " 'so',\n",
       " 'that',\n",
       " 'you',\n",
       " 'can',\n",
       " 'never',\n",
       " 'get',\n",
       " 'to',\n",
       " 'know',\n",
       " 'anyone',\n",
       " 'or',\n",
       " 'become',\n",
       " 'friendly',\n",
       " 'with',\n",
       " 'them.',\n",
       " 'It',\n",
       " 'can',\n",
       " 'all',\n",
       " 'go',\n",
       " 'to',\n",
       " 'Hell!\"',\n",
       " 'He',\n",
       " 'felt',\n",
       " 'a',\n",
       " 'slight',\n",
       " 'itch',\n",
       " 'up',\n",
       " 'on',\n",
       " 'his',\n",
       " 'belly;',\n",
       " 'pushed',\n",
       " 'himself',\n",
       " 'slowly',\n",
       " 'up',\n",
       " 'on',\n",
       " 'his',\n",
       " 'back',\n",
       " 'towards',\n",
       " 'the',\n",
       " 'headboard',\n",
       " 'so',\n",
       " 'that',\n",
       " 'he',\n",
       " 'could',\n",
       " 'lift',\n",
       " 'his',\n",
       " 'head',\n",
       " 'better;',\n",
       " 'found',\n",
       " 'where',\n",
       " 'the',\n",
       " 'itch',\n",
       " 'was,',\n",
       " 'and',\n",
       " 'saw',\n",
       " 'that',\n",
       " 'it',\n",
       " 'was',\n",
       " 'covered',\n",
       " 'with',\n",
       " 'lots',\n",
       " 'of',\n",
       " 'little',\n",
       " 'white',\n",
       " 'spots',\n",
       " 'which',\n",
       " 'he',\n",
       " \"didn't\",\n",
       " 'know',\n",
       " 'what',\n",
       " 'to',\n",
       " 'make',\n",
       " 'of;',\n",
       " 'and',\n",
       " 'when',\n",
       " 'he',\n",
       " 'tried',\n",
       " 'to',\n",
       " 'feel',\n",
       " 'the',\n",
       " 'place',\n",
       " 'with',\n",
       " 'one',\n",
       " 'of',\n",
       " 'his',\n",
       " 'legs',\n",
       " 'he',\n",
       " 'drew',\n",
       " 'it',\n",
       " 'quickly',\n",
       " 'back',\n",
       " 'because',\n",
       " 'as',\n",
       " 'soon',\n",
       " 'as',\n",
       " 'he',\n",
       " 'touched',\n",
       " 'it',\n",
       " 'he',\n",
       " 'was',\n",
       " 'overcome',\n",
       " 'by',\n",
       " 'a',\n",
       " 'cold',\n",
       " 'shudder.',\n",
       " 'He',\n",
       " 'slid',\n",
       " 'back',\n",
       " 'into',\n",
       " 'his',\n",
       " 'former',\n",
       " 'position.',\n",
       " '\"Getting',\n",
       " 'up',\n",
       " 'early',\n",
       " 'all',\n",
       " 'the',\n",
       " 'time\",',\n",
       " 'he',\n",
       " 'thought,',\n",
       " '\"it',\n",
       " 'makes',\n",
       " 'you',\n",
       " 'stupid.',\n",
       " \"You've\",\n",
       " 'got',\n",
       " 'to',\n",
       " 'get',\n",
       " 'enough',\n",
       " 'sleep.',\n",
       " 'Other',\n",
       " 'travelling',\n",
       " 'salesmen',\n",
       " 'live',\n",
       " 'a',\n",
       " 'life',\n",
       " 'of',\n",
       " 'luxury.',\n",
       " 'For',\n",
       " 'instance,',\n",
       " 'whenever',\n",
       " 'I',\n",
       " 'go',\n",
       " 'back',\n",
       " 'to',\n",
       " 'the',\n",
       " 'guest',\n",
       " 'house',\n",
       " 'during',\n",
       " 'the',\n",
       " 'morning',\n",
       " 'to',\n",
       " 'copy',\n",
       " 'out',\n",
       " 'the',\n",
       " 'contract,',\n",
       " 'these',\n",
       " 'gentlemen',\n",
       " 'are',\n",
       " 'always',\n",
       " 'still',\n",
       " 'sitting',\n",
       " 'there',\n",
       " 'eating',\n",
       " 'their',\n",
       " 'breakfasts.',\n",
       " 'I',\n",
       " 'ought',\n",
       " 'to',\n",
       " 'just',\n",
       " 'try',\n",
       " 'that',\n",
       " 'with',\n",
       " 'my',\n",
       " 'boss;',\n",
       " \"I'd\",\n",
       " 'get',\n",
       " 'kicked',\n",
       " 'out',\n",
       " 'on',\n",
       " 'the',\n",
       " 'spot.',\n",
       " 'But',\n",
       " 'who',\n",
       " 'knows,',\n",
       " 'maybe',\n",
       " 'that',\n",
       " 'would',\n",
       " 'be',\n",
       " 'the',\n",
       " 'best',\n",
       " 'thing',\n",
       " 'for',\n",
       " 'me.',\n",
       " 'If',\n",
       " 'I',\n",
       " \"didn't\",\n",
       " 'have',\n",
       " 'my',\n",
       " 'parents',\n",
       " 'to',\n",
       " 'think',\n",
       " 'about',\n",
       " \"I'd\",\n",
       " 'have',\n",
       " 'given',\n",
       " 'in',\n",
       " 'my',\n",
       " 'notice',\n",
       " 'a',\n",
       " 'long',\n",
       " 'time',\n",
       " 'ago,',\n",
       " \"I'd\",\n",
       " 'have',\n",
       " 'gone',\n",
       " 'up',\n",
       " 'to',\n",
       " 'the',\n",
       " 'boss',\n",
       " 'and',\n",
       " 'told',\n",
       " 'him',\n",
       " 'just',\n",
       " 'what',\n",
       " 'I',\n",
       " 'think,',\n",
       " 'tell',\n",
       " 'him',\n",
       " 'everything',\n",
       " 'I',\n",
       " 'would,',\n",
       " 'let',\n",
       " 'him',\n",
       " 'know',\n",
       " 'just',\n",
       " 'what',\n",
       " 'I',\n",
       " 'feel.',\n",
       " \"He'd\",\n",
       " 'fall',\n",
       " 'right',\n",
       " 'off',\n",
       " 'his',\n",
       " 'desk!',\n",
       " 'And',\n",
       " \"it's\",\n",
       " 'a',\n",
       " 'funny',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'business',\n",
       " 'to',\n",
       " 'be',\n",
       " 'sitting',\n",
       " 'up',\n",
       " 'there',\n",
       " 'at',\n",
       " 'your',\n",
       " 'desk,',\n",
       " 'talking',\n",
       " 'down',\n",
       " 'at',\n",
       " 'your',\n",
       " 'subordinates',\n",
       " 'from',\n",
       " 'up',\n",
       " 'there,',\n",
       " 'especially',\n",
       " 'when',\n",
       " 'you',\n",
       " 'have',\n",
       " 'to',\n",
       " 'go',\n",
       " 'right',\n",
       " 'up',\n",
       " 'close',\n",
       " 'because',\n",
       " 'the',\n",
       " 'boss',\n",
       " 'is',\n",
       " 'hard',\n",
       " 'of',\n",
       " 'hearing.',\n",
       " 'Well,',\n",
       " \"there's\",\n",
       " 'still',\n",
       " 'some',\n",
       " 'hope;',\n",
       " 'once',\n",
       " \"I've\",\n",
       " 'got',\n",
       " 'the',\n",
       " 'money',\n",
       " 'together',\n",
       " 'to',\n",
       " 'pay',\n",
       " 'off',\n",
       " 'my',\n",
       " \"parents'\",\n",
       " 'debt',\n",
       " 'to',\n",
       " 'him',\n",
       " '-',\n",
       " 'another',\n",
       " 'five',\n",
       " 'or',\n",
       " 'six',\n",
       " 'years',\n",
       " 'I',\n",
       " 'suppose',\n",
       " '-',\n",
       " \"that's\",\n",
       " 'definitely',\n",
       " 'what',\n",
       " \"I'll\",\n",
       " 'do.',\n",
       " \"That's\",\n",
       " 'when',\n",
       " \"I'll\",\n",
       " 'make',\n",
       " 'the',\n",
       " 'big',\n",
       " 'change.',\n",
       " 'First',\n",
       " 'of',\n",
       " 'all',\n",
       " 'though,',\n",
       " \"I've\",\n",
       " 'got',\n",
       " 'to',\n",
       " 'get',\n",
       " 'up,',\n",
       " 'my',\n",
       " 'train',\n",
       " 'leaves',\n",
       " 'at',\n",
       " 'five.\"',\n",
       " 'And',\n",
       " 'he',\n",
       " 'looked',\n",
       " 'over',\n",
       " 'at',\n",
       " 'the',\n",
       " 'alarm',\n",
       " 'clock,',\n",
       " 'ticking',\n",
       " 'on',\n",
       " 'the',\n",
       " 'chest',\n",
       " 'of',\n",
       " 'drawers.',\n",
       " '\"God',\n",
       " 'in',\n",
       " 'Heaven!\"',\n",
       " 'he',\n",
       " 'thought.',\n",
       " 'It',\n",
       " 'was',\n",
       " 'half',\n",
       " 'past',\n",
       " 'six',\n",
       " 'and',\n",
       " 'the',\n",
       " 'hands',\n",
       " 'were',\n",
       " 'quietly',\n",
       " 'moving',\n",
       " 'forwards,',\n",
       " 'it',\n",
       " 'was',\n",
       " 'even',\n",
       " 'later',\n",
       " 'than',\n",
       " 'half',\n",
       " 'past,',\n",
       " 'more',\n",
       " 'like',\n",
       " 'quarter',\n",
       " 'to',\n",
       " 'seven.',\n",
       " 'Had',\n",
       " 'the',\n",
       " 'alarm',\n",
       " 'clock',\n",
       " 'not',\n",
       " 'rung?',\n",
       " 'He',\n",
       " 'could',\n",
       " 'see',\n",
       " 'from',\n",
       " 'the',\n",
       " 'bed',\n",
       " 'that',\n",
       " 'it',\n",
       " 'had',\n",
       " 'been',\n",
       " 'set',\n",
       " 'for',\n",
       " 'four',\n",
       " \"o'clock\",\n",
       " 'as',\n",
       " 'it',\n",
       " 'should',\n",
       " 'have',\n",
       " 'been;',\n",
       " 'it',\n",
       " 'certainly',\n",
       " 'must',\n",
       " 'have',\n",
       " 'rung.',\n",
       " 'Yes,',\n",
       " 'but',\n",
       " 'was',\n",
       " 'it',\n",
       " 'possible',\n",
       " 'to',\n",
       " 'quietly',\n",
       " 'sleep',\n",
       " 'through',\n",
       " 'that',\n",
       " 'furniture-rattling',\n",
       " 'noise?',\n",
       " 'True,',\n",
       " 'he',\n",
       " 'had',\n",
       " 'not',\n",
       " 'slept',\n",
       " 'peacefully,',\n",
       " 'but',\n",
       " 'probably',\n",
       " 'all',\n",
       " 'the',\n",
       " 'more',\n",
       " 'deeply',\n",
       " 'because',\n",
       " 'of',\n",
       " 'that.',\n",
       " 'What',\n",
       " 'should',\n",
       " 'he',\n",
       " 'do',\n",
       " 'now?',\n",
       " 'The',\n",
       " 'next',\n",
       " 'train',\n",
       " 'went',\n",
       " 'at',\n",
       " 'seven;',\n",
       " 'if',\n",
       " 'he',\n",
       " 'were',\n",
       " 'to',\n",
       " 'catch',\n",
       " 'that',\n",
       " 'he',\n",
       " 'would',\n",
       " 'have',\n",
       " 'to',\n",
       " 'rush',\n",
       " 'like',\n",
       " 'mad',\n",
       " 'and',\n",
       " 'the',\n",
       " 'collection',\n",
       " 'of',\n",
       " 'samples',\n",
       " 'was',\n",
       " 'still',\n",
       " 'not',\n",
       " 'packed,',\n",
       " 'and',\n",
       " 'he',\n",
       " 'did',\n",
       " 'not',\n",
       " 'at',\n",
       " 'all',\n",
       " 'feel',\n",
       " 'particularly',\n",
       " 'fresh',\n",
       " 'and',\n",
       " 'lively.',\n",
       " 'And',\n",
       " 'even',\n",
       " 'if',\n",
       " 'he',\n",
       " 'did',\n",
       " 'catch',\n",
       " 'the',\n",
       " 'train',\n",
       " 'he',\n",
       " 'would',\n",
       " 'not',\n",
       " 'avoid',\n",
       " 'his',\n",
       " \"boss's\",\n",
       " 'anger',\n",
       " 'as',\n",
       " 'the',\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sometimes text data may contain non-printable characters\n",
    "re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "result = [re_print.sub('', w) for w in words]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01846415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï»¿the', 'project', 'gutenberg', 'ebook', 'of', 'metamorphosis,', 'by', 'franz', 'kafka', 'translated', 'by', 'david', 'wyllie.', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever.', 'you', 'may', 'copy', 'it,', 'give', 'it', 'away', 'or', 're-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'www.gutenberg.org', '**', 'this', 'is', 'a', 'copyrighted', 'project', 'gutenberg', 'ebook,', 'details', 'below', '**', '**', 'please', 'follow', 'the', 'copyright', 'guidelines', 'in', 'this', 'file.', '**', 'title:', 'metamorphosis', 'author:', 'franz', 'kafka', 'translator:', 'david', 'wyllie', 'release', 'date:', 'august', '16,', '2005', '[ebook', '#5200]', 'first', 'posted:', 'may', '13,', '2002', 'last', 'updated:']\n"
     ]
    }
   ],
   "source": [
    "# Normalizing Case\n",
    "filename = 'pg5200.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words by white space\n",
    "words = text.split()\n",
    "# convert to lower case\n",
    "words = [word.lower() for word in words]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b09b1f",
   "metadata": {},
   "source": [
    "## Tokenization and Cleaning with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a614e01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7d1f3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿The Project Gutenberg EBook of Metamorphosis, by Franz Kafka\n",
      "Translated by David Wyllie.\n"
     ]
    }
   ],
   "source": [
    "#Split into Sentences\n",
    "from nltk import sent_tokenize\n",
    "# load data\n",
    "filename = 'pg5200.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32822c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï', '»', '¿The', 'Project', 'Gutenberg', 'EBook', 'of', 'Metamorphosis', ',', 'by', 'Franz', 'Kafka', 'Translated', 'by', 'David', 'Wyllie', '.', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', '.', 'You', 'may', 'copy', 'it', ',', 'give', 'it', 'away', 'or', 're-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this', 'eBook', 'or', 'online', 'at', 'www.gutenberg.org', '*', '*', 'This', 'is', 'a', 'COPYRIGHTED', 'Project', 'Gutenberg', 'eBook', ',', 'Details', 'Below', '*', '*', '*', '*', 'Please', 'follow', 'the', 'copyright', 'guidelines', 'in', 'this', 'file', '.', '*', '*', 'Title', ':', 'Metamorphosis', 'Author', ':', 'Franz', 'Kafka', 'Translator', ':', 'David']\n"
     ]
    }
   ],
   "source": [
    "#Split into Words\n",
    "from nltk.tokenize import word_tokenize\n",
    "# load data\n",
    "filename = 'pg5200.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "737f3cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï', 'Project', 'Gutenberg', 'EBook', 'of', 'Metamorphosis', 'by', 'Franz', 'Kafka', 'Translated', 'by', 'David', 'Wyllie', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'You', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this', 'eBook', 'or', 'online', 'at', 'This', 'is', 'a', 'COPYRIGHTED', 'Project', 'Gutenberg', 'eBook', 'Details', 'Below', 'Please', 'follow', 'the', 'copyright', 'guidelines', 'in', 'this', 'file', 'Title', 'Metamorphosis', 'Author', 'Franz', 'Kafka', 'Translator', 'David', 'Wyllie', 'Release', 'Date', 'August', 'EBook', 'First', 'posted', 'May', 'Last', 'updated', 'May', 'Language', 'English', 'START', 'OF', 'THIS', 'PROJECT', 'GUTENBERG', 'EBOOK', 'METAMORPHOSIS', 'Copyright']\n"
     ]
    }
   ],
   "source": [
    "#Filter Out Punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "# load data\n",
    "filename = 'pg5200.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words\n",
    "tokens = word_tokenize(text)\n",
    "# remove all tokens that are not alphabetic\n",
    "words = [word for word in tokens if word.isalpha()]\n",
    "print(words[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd856626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#Filter out Stop Words (and Pipeline)\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0480a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï', 'project', 'gutenberg', 'ebook', 'metamorphosis', 'franz', 'kafka', 'translated', 'david', 'wyllie', 'ebook', 'use', 'anyone', 'anywhere', 'cost', 'almost', 'restrictions', 'whatsoever', 'may', 'copy', 'give', 'away', 'reuse', 'terms', 'project', 'gutenberg', 'license', 'included', 'ebook', 'online', 'wwwgutenbergorg', 'copyrighted', 'project', 'gutenberg', 'ebook', 'details', 'please', 'follow', 'copyright', 'guidelines', 'file', 'title', 'metamorphosis', 'author', 'franz', 'kafka', 'translator', 'david', 'wyllie', 'release', 'date', 'august', 'ebook', 'first', 'posted', 'may', 'last', 'updated', 'may', 'language', 'english', 'start', 'project', 'gutenberg', 'ebook', 'metamorphosis', 'copyright', 'c', 'david', 'wyllie', 'metamorphosis', 'franz', 'kafka', 'translated', 'david', 'wyllie', 'one', 'morning', 'gregor', 'samsa', 'woke', 'troubled', 'dreams', 'found', 'transformed', 'bed', 'horrible', 'vermin', 'lay', 'armourlike', 'back', 'lifted', 'head', 'little', 'could', 'see', 'brown', 'belly', 'slightly', 'domed']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# load data\n",
    "filename = 'pg5200.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words\n",
    "tokens = word_tokenize(text)\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "# prepare regex for char filtering\n",
    "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "# remove punctuation from each word\n",
    "stripped = [re_punc.sub('', w) for w in tokens]\n",
    "# remove remaining tokens that are not alphabetic\n",
    "words = [word for word in stripped if word.isalpha()]\n",
    "# filter out stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [w for w in words if not w in stop_words]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07649c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï', '»', '¿the', 'project', 'gutenberg', 'ebook', 'of', 'metamorphosi', ',', 'by', 'franz', 'kafka', 'translat', 'by', 'david', 'wylli', '.', 'thi', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyon', 'anywher', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrict', 'whatsoev', '.', 'you', 'may', 'copi', 'it', ',', 'give', 'it', 'away', 'or', 're-us', 'it', 'under', 'the', 'term', 'of', 'the', 'project', 'gutenberg', 'licens', 'includ', 'with', 'thi', 'ebook', 'or', 'onlin', 'at', 'www.gutenberg.org', '*', '*', 'thi', 'is', 'a', 'copyright', 'project', 'gutenberg', 'ebook', ',', 'detail', 'below', '*', '*', '*', '*', 'pleas', 'follow', 'the', 'copyright', 'guidelin', 'in', 'thi', 'file', '.', '*', '*', 'titl', ':', 'metamorphosi', 'author', ':', 'franz', 'kafka', 'translat', ':', 'david']\n"
     ]
    }
   ],
   "source": [
    "# Stem Words\": Stemming refers to the process of reducing each word to its root or base.\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "# load data\n",
    "filename = 'pg5200.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words\n",
    "tokens = word_tokenize(text)\n",
    "# stemming of words\n",
    "porter = PorterStemmer()\n",
    "stemmed = [porter.stem(word) for word in tokens]\n",
    "print(stemmed[:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1cacf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Additional Text Cleaning Considerations\n",
    "# Handling large documents and large collections of text documents that do not fit into memory.\n",
    "# Extracting text from markup like HTML, PDF, or other structured document formats.\n",
    "# Transliteration of characters from other languages into English.\n",
    "# Decoding Unicode characters into a normalized form, such as UTF8.\n",
    "# Handling of domain specific words, phrases, and acronyms.\n",
    "# Handling or removing numbers, such as dates and amounts.\n",
    "# Locating and correcting common typos and misspellings.\n",
    "# And much more...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3623dfd8",
   "metadata": {},
   "source": [
    "## How to Prepare Text Data with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0afaf713",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Bag-of-Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cac5553f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "(1, 8)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[1 1 1 1 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "#Word Counts with CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15f2a0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "022fa75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# encode another document\n",
    "text2 = [\"the puppy\"]\n",
    "vector = vectorizer.transform(text2)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "42c35398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "[1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.        ]\n",
      "(1, 8)\n",
      "[[0.36388646 0.27674503 0.27674503 0.36388646 0.36388646 0.36388646\n",
      "  0.36388646 0.42983441]]\n"
     ]
    }
   ],
   "source": [
    "#Word Frequencies with TfidfVectorizer\n",
    "# Term Frequency: This summarizes how often a given word appears within a document.\n",
    "# Inverse Document Frequency: This downscales words that appear a lot across documents\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "\"The dog.\",\n",
    "\"The fox\"]\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)\n",
    "# encode document\n",
    "vector = vectorizer.transform([text[0]])\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "294898db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hashing with HashingVectorizer\n",
    "#Counts and frequencies can be very useful, but one limitation of these methods is that the vocabulary can become very large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92c98b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20)\n",
      "[[ 0.          0.          0.          0.          0.          0.33333333\n",
      "   0.         -0.33333333  0.33333333  0.          0.          0.33333333\n",
      "   0.          0.          0.         -0.33333333  0.          0.\n",
      "  -0.66666667  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# create the transform\n",
    "vectorizer = HashingVectorizer(n_features=20)\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92397499",
   "metadata": {},
   "source": [
    "## How to Prepare Text Data With Keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "da1b003f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\inkri\\.conda\\envs\\projects\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.6) or chardet (5.0.0)/charset_normalizer (2.0.5) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "#Split Words with text to word sequence\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "# define the document\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "# tokenize the document\n",
    "result = text_to_word_sequence(text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "86b1043d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# Encoding with one hot\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "# define the document\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "# estimate the size of the vocabulary\n",
    "words = set(text_to_word_sequence(text))\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "da4be090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[7, 2, 6, 3, 1, 4, 7, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "# define the document\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "# estimate the size of the vocabulary\n",
    "words = set(text_to_word_sequence(text))\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)\n",
    "# integer encode the document\n",
    "result = one_hot(text, round(vocab_size*1.3))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "353453a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[6, 4, 1, 2, 7, 5, 6, 2, 6]\n"
     ]
    }
   ],
   "source": [
    "#Hash Encoding with hashing trick\n",
    "from keras.preprocessing.text import hashing_trick\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "# define the document\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "# estimate the size of the vocabulary\n",
    "words = set(text_to_word_sequence(text))\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)\n",
    "# integer encode the document\n",
    "result = hashing_trick(text, round(vocab_size*1.3), hash_function='md5')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3c9b63c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer API\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "# define 5 documents\n",
    "docs = ['Well done!',\n",
    "'Good work',\n",
    "'Great effort',\n",
    "'nice work',\n",
    "'Excellent!']\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "624a2817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n",
      "5\n",
      "{'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8}\n",
      "defaultdict(<class 'int'>, {'done': 1, 'well': 1, 'good': 1, 'work': 2, 'great': 1, 'effort': 1, 'nice': 1, 'excellent': 1})\n"
     ]
    }
   ],
   "source": [
    "# summarize what was learned\n",
    "# word counts: A dictionary of words and their counts.\n",
    "# word docs: An integer count of the total number of documents that were used to fit the Tokenizer.\n",
    "# word index: A dictionary of words and their uniquely assigned integers.\n",
    "# document count: A dictionary of words and how many documents each appeared in.\n",
    "    \n",
    "print(t.word_counts)\n",
    "print(t.document_count)\n",
    "print(t.word_index)\n",
    "print(t.word_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "797a2ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n",
      "5\n",
      "{'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8}\n",
      "defaultdict(<class 'int'>, {'done': 1, 'well': 1, 'good': 1, 'work': 2, 'great': 1, 'effort': 1, 'nice': 1, 'excellent': 1})\n",
      "[[0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "# define 5 documents\n",
    "docs = ['Well done!',\n",
    "'Good work',\n",
    "'Great effort',\n",
    "'nice work',\n",
    "'Excellent!']\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs)\n",
    "# summarize what was learned\n",
    "# binary: Whether or not each word is present in the document. This is the default.\n",
    "# count: The count of each word in the document.\n",
    "# tfidf: The Text Frequency-Inverse DocumentFrequency (TF-IDF) scoring for each word in the document.\n",
    "# freq: The frequency of each word as a ratio of words within each document.\n",
    "\n",
    "print(t.word_counts)\n",
    "print(t.document_count)\n",
    "print(t.word_index)\n",
    "print(t.word_docs)\n",
    "# integer encode documents\n",
    "encoded_docs = t.texts_to_matrix(docs, mode='count')\n",
    "print(encoded_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d41c58a",
   "metadata": {},
   "source": [
    "# Bag-of-Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0205e16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the Bag-of-Words Model\n",
    "\n",
    "#Step 1: Collect Data\n",
    "#Step 2: Design the Vocabulary\n",
    "#Step 3: Create Document Vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "843c51a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Managing Vocabulary\n",
    "#There are simple text cleaning techniques that can be used as a first step, such as:\n",
    "# Ignoring case.\n",
    "# Ignoring punctuation.\n",
    "# Ignoring frequent words that don’t contain much information, called stop words, like a, of, etc.\n",
    "# Fixing misspelled words.\n",
    "# Reducing words to their stem (e.g. play from playing) using stemming algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "28b032ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scoring Words\n",
    "# Counts. Count the number of times each word appears in a document.\n",
    "# Frequencies. Calculate the frequency that each word appears in a document out of all the words in the document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "73b32808",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Hashing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "580fc564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF:\n",
    "# Term Frequency: is a scoring of the frequency of the word in the current document.\n",
    "# Inverse Document Frequency: is a scoring of how rare the word is across documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d4e702f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limitations of Bag-of-Words\n",
    "#Vocabulary:\n",
    "#Sparsity:\n",
    "#Meaning:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c235fd",
   "metadata": {},
   "source": [
    "## What is word embedding?\n",
    "### Word embedding or word vector is an approach with which we represent documents and words. It is defined as a numeric vector input that allows words with similar meanings to have the same representation. It can approximate meaning and represent a word in a lower dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe7c6a5",
   "metadata": {},
   "source": [
    "#### Different types of Word Embeddings\n",
    "\n",
    "#### 1. Frequency based Embedding\n",
    "##### Count Vector\n",
    "#####  TF-IDF Vector\n",
    "#####  Co-Occurrence Vector\n",
    "#### One-hot Encoding (OHE)\n",
    "#### N-grams\n",
    "\n",
    "#### 2. Prediction based Embedding\n",
    "#####  CBOW\n",
    "##### Skip-Gram\n",
    "\n",
    "#### 3. gloVe(Global Vector)\n",
    "\n",
    "#### 4. BERT (Bidirectional encoder representations from transformers)\n",
    "\n",
    "\n",
    "### Notes: \n",
    "### Word embeddings can ‌train deep learning models like GRU, LSTM, and Transformers, which have been successful in NLP tasks such as sentiment classification, name entity recognition, speech recognition, etc.\n",
    "\n",
    "### Bag of words: Extracts features from the text\n",
    "### TF-IDF: Information retrieval, keyword extraction\n",
    "### Word2Vec: Semantic analysis task\n",
    "### GloVe: Word analogy, named entity recognition tasks\n",
    "### BERT: language translation, question answering system\n",
    "\n",
    "\n",
    "\n",
    "#### Reading:\n",
    "#### https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n",
    "#### https://medium.com/@hari4om/word-embedding-d816f643140\n",
    "#### https://www.analyticsvidhya.com/blog/2021/06/part-5-step-by-step-guide-to-master-nlp-text-vectorization-approaches/ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4148f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

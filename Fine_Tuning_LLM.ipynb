{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "063098f6-fd2e-427c-b74a-e32a23e1a96d",
   "metadata": {},
   "source": [
    "## Custom_Sentiment_Analysis\n",
    "### DistilBERT, short for \"Distilled BERT,\" is a smaller and faster version of the BERT (Bidirectional Encoder Representations from Transformers) model.\n",
    "### DistilBERT, a compressed version of BERT, employs a 6-layer Transformer architecture with 768 hidden units, reducing computational complexity while maintaining competitive performance. Utilizing knowledge distillation, it emulates the behavior of BERT, achieving faster inference times and reduced resource requirements. Pre-trained DistilBERT models are available for various languages and tasks, enabling fine-tuning on specific datasets for tasks like text classification and sentiment analysis. Despite its smaller size, DistilBERT offers an efficient solution for natural language processing applications, delivering a balance between model size, speed, and performance in resource-constrained environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f672eca-3b7f-44ba-b4ea-403880d9e3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Abhishek_Jaiswal\\AppData\\Local\\miniconda3\\envs\\mlenv\\Lib\\site-packages\\transformers\\trainer_tf.py:118: FutureWarning: The class `TFTrainer` is deprecated and will be removed in version 5 of Transformers. We recommend using native Keras instead, by calling methods like `fit()` and `predict()` directly on the model object. Detailed examples of the Keras style can be found in our examples at https://github.com/huggingface/transformers/tree/main/examples/tensorflow\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast, TFDistilBertForSequenceClassification, TFTrainer, TFTrainingArguments\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('SMSSpamCollection.txt', sep='\\t', names=[\"label\", \"message\"])\n",
    "df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "\n",
    "X = list(df['message'])\n",
    "y = list(df['label'])\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
    "\n",
    "# Define and load the tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenize the training and testing data\n",
    "train_encodings = tokenizer(X_train, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(X_test, truncation=True, padding=True)\n",
    "\n",
    "# Create TensorFlow datasets from tokenized data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    y_train\n",
    "))\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    y_test\n",
    "))\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TFTrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=2,              # total number of training epochs\n",
    "    per_device_train_batch_size=8,   # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    eval_steps=100                   # evaluation steps, set it to an appropriate value\n",
    ")\n",
    "\n",
    "# Instantiate the model within the training scope\n",
    "with training_args.strategy.scope():\n",
    "    model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = TFTrainer(\n",
    "    model=model,                    # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,             # training arguments\n",
    "    train_dataset=train_dataset,    # training dataset\n",
    "    eval_dataset=test_dataset       # evaluation dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7cff80b-1ae3-4648-9047-df35c75ff916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[954   1]\n",
      " [  4 156]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       955\n",
      "           1       0.99      0.97      0.98       160\n",
      "\n",
      "    accuracy                           1.00      1115\n",
      "   macro avg       0.99      0.99      0.99      1115\n",
      "weighted avg       1.00      1.00      1.00      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# Extract predicted labels\n",
    "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, predicted_labels)\n",
    "\n",
    "# Display confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Calculate classification report\n",
    "class_report = classification_report(y_test, predicted_labels)\n",
    "\n",
    "# Display classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b5ec901-02bf-4b11-83fb-9f81e3e658cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at saved_model were not used when initializing TFDistilBertForSequenceClassification: ['dropout_291']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at saved_model and are newly initialized: ['dropout_311']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model.save_pretrained(\"saved_model\")\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = TFDistilBertForSequenceClassification.from_pretrained(\"saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "150c82d5-6744-4c1f-9b73-c3a6bcaecbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: 1\n"
     ]
    }
   ],
   "source": [
    "# Define a function for making predictions on new text\n",
    "def predict_text(text):\n",
    "    # Tokenize the text\n",
    "    inputs = tokenizer(text, truncation=True, padding=True, return_tensors=\"tf\")\n",
    "    \n",
    "    # Make prediction\n",
    "    logits = loaded_model(inputs)[0]\n",
    "    \n",
    "    # Get predicted label\n",
    "    predicted_label = np.argmax(logits, axis=1).squeeze()\n",
    "    \n",
    "    return predicted_label\n",
    "\n",
    "# Example usage\n",
    "new_text = \"REMINDER FROM O2: To get 2.50 pounds free call credit and details of great offers pls reply 2 this text with your valid name, house no and postcode\"\n",
    "predicted_label = predict_text(new_text)\n",
    "print(\"Predicted label:\", predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbf5bca9-3e01-43f1-afc1-4a4e157c27c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/justmarkham/DAT5/blob/master/data/SMSSpamCollection.txt\n",
    "#https://huggingface.co/docs/transformers/training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbe15c9-54e9-4d97-a740-80f700030f51",
   "metadata": {},
   "source": [
    "# EXAMPLE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b09bb1-7482-4f96-8969-63e7f0dc1cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c536b59-068e-4929-8de1-cb7ac5338ffb",
   "metadata": {},
   "source": [
    "# Transformers Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c3f1c7-5913-450b-bd0a-1eb79144c61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers Agents is an experimental API which is subject to change at any time. Results returned by the agents can vary as the APIs or underlying models are prone to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172a03ca-e2a2-4ebf-a367-61ee9668abab",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\"Caption the following image\", image=image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7b5d4a-b342-4a28-8307-67c96a183c00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

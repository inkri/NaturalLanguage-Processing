{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b9498a",
   "metadata": {},
   "source": [
    "## Answer Quality Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fbc8b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://kaushikshakkari.medium.com/open-domain-question-answering-series-part-4-answer-quality-metrics-evaluating-the-reader-ff7fa20736bf\n",
    "#https://www.kdnuggets.com/2020/04/simple-question-answering-systems-text-similarity-python.html\n",
    "#https://medium.com/@sthanikamsanthosh1994/understanding-bleu-and-rouge-score-for-nlp-evaluation-1ab334ecadcb#:~:text=While%20BLEU%20score%20is%20primarily,the%20reference%20translations%20or%20summaries.\n",
    "#https://medium.com/@mikeusru/common-metrics-for-evaluating-natural-language-processing-nlp-models-e84190063b5f\n",
    "#https://towardsdatascience.com/comprehensive-guide-to-ranking-evaluation-metrics-7d10382c1025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf2a452",
   "metadata": {},
   "source": [
    "### The Reader evaluation metrics can be divided into two categories:\n",
    "\n",
    "### Lexical or Keyword-based Evaluation Metrics\n",
    "### Neural Based Evaluation Metrics\n",
    "### Ranking Based Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117589d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a13c2292",
   "metadata": {},
   "source": [
    "## Lexical or Keyword-based Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9be25af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Lexical or Keyword-based Evaluation Metrics:\n",
    "\n",
    "#1.1 Exact Match (EM):\n",
    "#Exact Match is a strict evaluation metric that only gives two scores (0 or 1).\n",
    "\n",
    "#1.2 F1-Score:\n",
    "#F1-Score is a looser metric than Exact Match; it considers the average overlap between the answer provided by the annotator and the predicted answer.\n",
    "#Before calculating the score, we may need to perform some preprocessing on answers like converting words to lower cases, stemming or lemmatization, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcc602d",
   "metadata": {},
   "source": [
    "### F1= (2*precision*recall)/(precision+recall)\n",
    "### precision = TP/(TP+FP)\n",
    "### recall=TP/(TP+FN)\n",
    "### True Positive (TP) is the number of words that overlap between the annotated label and the predicted answer.\n",
    "### False Positive (FP) is the number of words present in the predicted answer but missing in the annotated label. \n",
    "### False Negative (FN) is the number of words present in the annotated label but missing in the predicted answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "230a0c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's consider the below example,\n",
    "#Annotated Answer / Gold Standard Label: Anthony Edward Stark\n",
    "#Predicted Answer by model: Tony Stark\n",
    "#Exact Match Score: 0\n",
    "#F1-Score: 0.67 [TP = 1(Stark), FP = 1(Tony), FN = 2(Anthony, Edward) -> precision = 0.5, recall = 0.398]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e31923",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a897af12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-gram-based lexical matching algorithms\n",
    "#BLEU and ROUGE scores are valuable tools for evaluating the performance of NLP models in machine translation and text summarization tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31fc793",
   "metadata": {},
   "source": [
    "### While BLEU score is primarily used for machine translation tasks, ROUGE score is used for text summarization tasks. Both metrics rely on n-gram overlap to measure similarity between the machine-generated output and the reference translations or summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aef1db4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLEU (Bilingual Evaluation Understudy) Score:\n",
    "#BLEU score is a widely used metric for machine translation tasks, where the goal is to automatically translate text from one language to another. It was proposed as a way to assess the quality of machine-generated translations by comparing them to a set of reference translations provided by human translators.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89ee128",
   "metadata": {},
   "source": [
    "### The formula for BLEU score is as follows:\n",
    "\n",
    "### BLEU = BP * exp(∑ pn)\n",
    "\n",
    "### Where:\n",
    "\n",
    "### BP (Brevity Penalty) is a penalty term that adjusts the score for translations that are shorter than the reference translations. It is calculated as min(1, (reference_length / translated_length)), where reference_length is the total number of words in the reference translations, and translated_length is the total number of words in the machine-generated translation.\n",
    "### pn is the precision of n-grams, which is calculated as the number of n-grams that appear in both the machine-generated translation and the reference translations divided by the total number of n-grams in the machine-generated translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6fdb4da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [['this', 'is', 'a', 'test'], ['this', 'is' 'test']]\n",
    "candidate = ['this', 'is', 'a', 'test']\n",
    "score = sentence_bleu(reference, candidate)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a676674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 100.0000\n"
     ]
    }
   ],
   "source": [
    "#!pip install sacrebleu\n",
    "import sacrebleu\n",
    "\n",
    "# Example sentences\n",
    "reference = [\"The cat is on the mat\"]\n",
    "candidate = \"The cat is on the mat\"\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu = sacrebleu.raw_corpus_bleu([candidate], [reference])\n",
    "print(f\"BLEU Score: {bleu.score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b7fbf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROUGE (Recall-Oriented Understudy for Gisting Evaluation) Score:\n",
    "#ROUGE score is a set of metrics commonly used for text summarization tasks, where the goal is to automatically generate a concise summary of a longer text. ROUGE was designed to evaluate the quality of machine-generated summaries by comparing them to reference summaries provided by humans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aa43b1",
   "metadata": {},
   "source": [
    "### ROUGE = ∑ (Recall of n-grams)\n",
    "\n",
    "### Where:\n",
    "\n",
    "### Recall of n-grams is the number of n-grams that appear in both the machine-generated summary and the reference summaries divided by the total number of n-grams in the reference summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6fe8843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=0.75, recall=0.6666666666666666, fmeasure=0.7058823529411765),\n",
       " 'rougeL': Score(precision=0.625, recall=0.5555555555555556, fmeasure=0.5882352941176471)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install rouge-score\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score('The quick brown fox jumps over the lazy dog','The quick brown dog jumps on the log.')\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f34be0c",
   "metadata": {},
   "source": [
    "## Neural Based Evaluation Metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb9d591",
   "metadata": {},
   "source": [
    "### While lexical metrics like EM and F1 scores are keyword-based and focus on keyword match, neural-based metrics focus on the semantics or meanings of answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c129b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.1 BertScore:\n",
    "#BertScore is based on the BERT language model. It computes the contextual embeddings for each word in answer labels and predicted answers. Later, an algorithm like cosine similarity is used to calculate the contextual similarity between each word in answer labels and each word in predicted answers. The highest cosine similarity between a token from label answer and a token from annotations is considered as BertScore.\n",
    "\n",
    "#2.2 Bi-Encoder Score:\n",
    "#Bi-Encoder Score is based on sentence transformers architecture. It uses two language models to separately calculate embeddings for predicted answers and answer labels. Later cosine similarity is used to calculate the score between contextual embeddings. Before calculating embeddings, two language models are trained on the multi-lingual paraphrase dataset and STS benchmark dataset. \n",
    "\n",
    "#2.3 Semantic Similarity:\n",
    "#Semantic Similarity or Semantic Answer Similarity (SAS) uses the “cross-encoder/stsb-roberta-large” language model, which has been trained on the STS benchmark dataset. Unlike Bi-Encoder where two separate models are used, SAS uses a cross-encoder architecture where a predicted answer and a label are separated by a special token to calculate the score. Among all neural-based metrics, cross-encoder model metrics have relatively the strongest correlation with human judgment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6627f993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at T-Systems-onsite/cross-en-de-roberta-sentence-transformer and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.5192396], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bi-Encoder Score\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "model = CrossEncoder('T-Systems-onsite/cross-en-de-roberta-sentence-transformer', max_length=512)\n",
    "scores = model.predict([['atleast 1000', 'four thousand']])\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de07e0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading config.json: 100%|████████████████████████████████████████████████████████████████| 629/629 [00:00<?, ?B/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████████████████████████████████████████| 1.42G/1.42G [02:19<00:00, 10.2MB/s]\n",
      "Downloading tokenizer_config.json: 100%|██████████████████████████████████████████████████████| 139/139 [00:00<?, ?B/s]\n",
      "Downloading vocab.json: 100%|███████████████████████████████████████████████████████| 899k/899k [00:00<00:00, 4.15MB/s]\n",
      "Downloading merges.txt: 100%|████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 632kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|█████████████████████████████████████████████| 772/772 [00:00<00:00, 775kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.29968646]\n"
     ]
    }
   ],
   "source": [
    "#Semantic Similarity or Semantic Answer Similarity (SAS)\n",
    "\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "model = CrossEncoder('cross-encoder/stsb-roberta-large')\n",
    "scores = model.predict([[\"Thirty Bucks\", \"30 $\"]])\n",
    "print (scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f780609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ded6799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example:1\n",
    "#Annotated Answer / Gold Standard Label: I love you 3000\n",
    "#Predicted Answer by model: likes and adores very much\n",
    "#Exact Match Score: 0\n",
    "#F1-Score: undefined or generally considered as 0 [TP = 0, FP = 5, FN = 4-> precision = 0, recall = 0]\n",
    "#Bi-Encoder Score: 0.151\n",
    "#Semantic Answer Similarity: 0.482"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3416486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example: 2\n",
    "#Annotated Answer / Gold Standard Label: Thirty Bucks\n",
    "#Predicted Answer by model: 30 $\n",
    "#Exact Match Score: 0\n",
    "#F1-Score: undefined or generally considered as 0 [TP = 0, FP = 2, FN = 2-> precision = 0, recall = 0]\n",
    "#Bi-Encoder Score: 0.869\n",
    "#Semantic Answer Similarity: 0.493"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0173c86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
